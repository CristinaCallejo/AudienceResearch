{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6527421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2 # for OpenCV bindings.\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "import matplotlib.pyplot as plt\n",
    "import face_recognition\n",
    "from pathlib import Path\n",
    "import os\n",
    "import imutils # for OpenCV convenience functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca50ed",
   "metadata": {},
   "source": [
    "## HAAR CASCADE CLASSIFIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3bef5b",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "823be13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIS: try out cnn from opencv! check: first one is normal haar, second is deeplearninh improved\n",
    "    # https://www.pyimagesearch.com/2021/04/05/opencv-face-detection-with-haar-cascades/\n",
    "    # https://www.pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('../src/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('../src/haarcascade_eye.xml')\n",
    "smile_cascade = cv2.CascadeClassifier('../src/haarcascade_smile.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b5bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "HaarCascade Classifiers:\n",
    "    \n",
    "    If faces are found, returns the positions of detected faces as Rect(x,y,w,h).\n",
    "\n",
    "    cv2.CascadeClassifier.detectMultiScale(\n",
    "        image[, scaleFactor[, minNeighbors[, flags[, minSize[, maxSize]]]]]) \n",
    "\n",
    "        image: Matrix of the type CV_8U containing an image where objects are detected.\n",
    "        scaleFactor (max recommendd: 1,4) : how much the image size is reduced at each image scale.\n",
    "            creates scale pyramid. For scaleFactor 1.03: using a small step for resizing,\n",
    "            i.e. reduce size by 3 %\n",
    "            --> increase the chance of a matching size with the model for detection,but it's expensive.\n",
    "        minNeighbors (recommended 3-6) : many rectangles (neighbors) need to be detected \n",
    "            for the window to be labeled a face.how many neighbors each candidate rectangle should have to retain it. \n",
    "            will affect the quality of the detected faces: \n",
    "            higher value results in less detections but with higher quality.\n",
    "            We're using 5 in the code.\n",
    "        flags : Parameter with the same meaning for an old cascade as in the function cvHaarDetectObjects. \n",
    "            Not used for a new cascade.\n",
    "        minSize : pixels(30x30 recommended) windows/objects minimum possible size. \n",
    "            Objects smaller than that are ignored.\n",
    "        maxSize : Maximum possible object size. \n",
    "            Objects larger than that are ignored.\n",
    "            \n",
    "    Haar cascades tend to be very sensitive to your choice\n",
    "    in detectMultiScale parameters. \n",
    "    The scaleFactor and minNeighbors being the ones you have to tune most often.\n",
    "\n",
    "\"\"\"\n",
    "# !!! Why, when and how to use Haar vs HOG + Linear SVM, SSD, YOLO + capturing from video implementation\n",
    "    # https://www.pyimagesearch.com/2021/04/05/opencv-face-detection-with-haar-cascades/\n",
    "# Params explained\n",
    "    # https://towardsdatascience.com/computer-vision-detecting-objects-using-haar-cascade-classifier-4585472829a9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341e03d5",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55af833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_demo = Path.cwd().parent/'demo'\n",
    "dir_demo_faces = Path.cwd().parent/'demo_faces'\n",
    "dir_imgs= Path.cwd().parent/'imgs'\n",
    "dir_imgs_faces = Path.cwd().parent/'imgs_faces'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2009ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_demo.parent/'demo_faces'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face_eyes_smile(pth):\n",
    "    counter_imgs = 0\n",
    "    counter_faces = 0\n",
    "    counter_smiles = 0\n",
    "    counter_eyes = 0\n",
    "\n",
    "    for file in sorted(pth.iterdir()):\n",
    "        if file.suffix != '.jpg':\n",
    "            pass\n",
    "        else:\n",
    "            counter_imgs += 1\n",
    "            print(file.name)\n",
    "            img = cv2.imread(str(file))\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            plt.imshow(img)\n",
    "\n",
    "            # FRONTAL FACE \n",
    "            \n",
    "            faces = face_cascade.detectMultiScale(\n",
    "                gray, \n",
    "                scaleFactor=1.06,                \n",
    "                minNeighbors=7,\n",
    "                minSize=(30, 30), \n",
    "                flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "            if faces is None:\n",
    "                print(\"No Face Found\")\n",
    "\n",
    "            for (fx,fy,fw,fh) in faces:\n",
    "                counter_faces += 1\n",
    "                roi_gray = gray[fy:fy+fh, fx:fx+fw] # region of interest for detection\n",
    "                roi_color = img[fy:fy+fh, fx:fx+fw] # region of interest for mapping rectangle\n",
    "                cv2.rectangle(\n",
    "                    img,\n",
    "                    (fx,fy),\n",
    "                    (fx+fw,fy+fh),\n",
    "                    #(127,0,255),\n",
    "                    (0,255,0),\n",
    "                    2)\n",
    "\n",
    "                # SMILES \n",
    "\n",
    "                smiles = smile_cascade.detectMultiScale(\n",
    "                    roi_gray, \n",
    "                    scaleFactor = 1.35, \n",
    "                    minNeighbors = 8)\n",
    "\n",
    "                for (sx, sy, sw, sh) in smiles:\n",
    "                    counter_smiles += 1\n",
    "                    cv2.rectangle(\n",
    "                        roi_color,\n",
    "                        (sx, sy),\n",
    "                        (sx + sw, sy + sh),\n",
    "                        #(255, 0, 130),\n",
    "                        #(0,220,80),\n",
    "                        (127,0,255),\n",
    "                        1)\n",
    "\n",
    "                # EYES\n",
    "\n",
    "                eyes = eye_cascade.detectMultiScale(\n",
    "                    roi_gray,\n",
    "                    scaleFactor=1.05,\n",
    "                    minNeighbors = 6)\n",
    "\n",
    "                for (ex,ey,ew,eh) in eyes:\n",
    "                    counter_eyes += 1\n",
    "                    cv2.rectangle(\n",
    "                        roi_color, \n",
    "                        (ex , ey),\n",
    "                        (ex + ew, ey + eh),\n",
    "                        (0,255,255),\n",
    "                        1)\n",
    "            \n",
    "                # save images with detected regions\n",
    "                file_to_save = file.name.replace(\".\",f\"_face{counter_faces}.\")\n",
    "                #cv2.imwrite(str(pth.parent/'demo_faces'/file_to_save),img)\n",
    "                cv2.imwrite(str(pth.parent/'demo_faces'/file_to_save),roi_color)\n",
    "                counter_imgs = 0\n",
    "                counter_faces = 0\n",
    "            # show the output frame\n",
    "            cv2.imshow(f\"img{file_to_save}\", img)\n",
    "            key = cv2.waitKey(3) & 0xFF\n",
    "\n",
    "        # if the `q` key was pressed, break from the loop\n",
    "            if key == ord(\"q\"):\n",
    "                # do a bit of cleanup\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "        \n",
    "    # do a bit of cleanup\n",
    "    cv2.destroyAllWindows()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d7bd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detect_face_eyes_smile(dir_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a37235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pink = [255, 153, 255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c01f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "orange = [255, 128, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172e7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "red = [255, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c38fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bright_green = [0, 255, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a4eca85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-7b98b03e07b0>:6: FutureWarning: The new recommended value for bg_label is 0. Until version 0.19, the default bg_label value is -1. From version 0.19, the bg_label default value will be 0. To avoid this warning, please explicitly set bg_label value.\n",
      "  rgb_image = color.label2rgb(label_image)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage import color,io\n",
    "import numpy as np\n",
    "from skimage import color, io\n",
    "\n",
    "label_image = np.random.randint(100, size=(10, 10))\n",
    "rgb_image = color.label2rgb(label_image)\n",
    "\n",
    "io.imshow(rgb_image)\n",
    "io.show()\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67423fc8",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee603cf9",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa58343",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b79c66d",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebbf94b",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db786694",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce07c9",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e558b68",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d88b470",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39489ec7",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7e0525",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1100ec8d",
   "metadata": {},
   "source": [
    "# FUNCTION ONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad0ed9",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289499a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openImageAndDetectFaces3(paaath):\n",
    "    \n",
    "    for file in sorted(paaath.iterdir()):\n",
    "        \n",
    "        if file.suffix == '.jpg':\n",
    "            \n",
    "            image = face_recognition.load_image_file(file)\n",
    "            face_locations = face_recognition.face_locations(image)\n",
    "            count = 0\n",
    "            \n",
    "            try:\n",
    "                #print(f\"I found {len(face_locations)} face(s) in photograph {file.name}.\")\n",
    "                #plt.subplot(121)\n",
    "                #plt.imshow(image)\n",
    "                \n",
    "                for feis in face_locations:\n",
    "                    count+=1\n",
    "                    top, right, bottom, left = feis\n",
    "                    face_image = image[top:bottom, left:right]\n",
    "                    face_image = cv2.cvtColor(face_image, cv2.COLOR_RGB2GRAY)\n",
    "                    pil_image = Image.fromarray(face_image)\n",
    "                    reshaped_image = modify_size(pil_image)\n",
    "\n",
    "                    file_face = file.name.replace(\".\",f\"_face{count}.\")\n",
    "                    file_pil = file.name.replace(\".\",f\"_pil{count}.\")\n",
    "                    file_reshaped = file.name.replace(\".\",f\"_reshaped{count}.\")\n",
    "                    cv2.imwrite(str(folder_faces/file_face), face_image)\n",
    "                    \n",
    "                    #fig, axarr = plt.subplots(len(face_locations),1)\n",
    "                    #plt.subplot(121)\n",
    "                    #plt.imshow(new_size(pil_image))\n",
    "                    #f.name.replace(\".\",f\"_face{count}.\")\n",
    "                    #cv2.imwrite(str(folder_faces/file_pil), pil_image)\n",
    "                    #cv2.imwrite(str(folder_faces/file_reshaped), reshaped_image)\n",
    "                    #cv2.imwrite(str(folder_faces/file.name)+f\"_face{count}\", face_image)\n",
    "                    #return new_size(pil_image)\n",
    "            \n",
    "            except ValueError as e:\n",
    "                print(f\"No face found\")\n",
    "                \n",
    "    return reshaped_image\n",
    "\n",
    "        \n",
    "def modify_size(img):\n",
    "    size=(48,48)\n",
    "    convert_from = img.resize(size)\n",
    "    face=np.asarray(convert_from)/255\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4dae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "face = openImageAndDetectFaces3(folder_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0206accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('say -v Jorge Ya he terminado');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5eab0f",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e63621",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093060a4",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbaf5b7",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab668d",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eacde9",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fc7955",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f026643",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be3b38f",
   "metadata": {},
   "source": [
    "# FUNCTION TWO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625fb5c6",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a63d900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_col = Image.open(pth)\n",
    "image_blues = cv2.imread(pth)\n",
    "image_greens = cv2.cvtColor(image_blues, cv2.COLOR_BGR2GRAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f720c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(image_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fd9801",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(pth)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# now we can try to detect faces\n",
    "faces = face_cascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=5,\n",
    "    minSize=(30, 30),\n",
    "    #flags = cv2.CASCADE_SCALE_IMAGE\n",
    ")\n",
    "\n",
    "for feis in faces:\n",
    "    print(feis)\n",
    "# Draw a rectangle around the faces and display on screen\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        #cv2.imshow(\"Faces found\", image)\n",
    "        #cv2.waitKey(0)\n",
    "        #cv2.destroyAllWindows()\n",
    "    #print(flags)\n",
    "        #cv2.resize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e5e353",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc4af68",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedda70e",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb9db86",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc190ad",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ceab81",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310b8a7b",
   "metadata": {},
   "source": [
    "### FUNCTION TWO-A ¿?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280fcc54",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca87a80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(image_blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b05b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(image_blues))\n",
    "print('len(image_blues): ',len(image_blues))\n",
    "print('image_blues.shape: ',image_blues.shape)\n",
    "print('len(image_blues)[0]: ',len(image_blues[0]))\n",
    "print('image_blues[0]: ',image_blues[0])\n",
    "print('image_blues[0][0]: ',image_blues[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e07c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_blue = face_cascade.detectMultiScale(\n",
    "    image_blues,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=5,\n",
    "    minSize=(30, 30),\n",
    "    #flags = cv2.CASCADE_SCALE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6d56aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(type(faces_blue))\n",
    "print('len(faces_blue): ',len(faces_blue))\n",
    "print('faces_blue.shape: ',faces_blue.shape)\n",
    "print('len(faces_blue)[0]: ',len(faces_blue[0]))\n",
    "print('faces_blue[0]: ',faces_blue[0])\n",
    "print('faces_blue: ',faces_blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x, y, w, h) in faces_blue:\n",
    "    rec_blue = cv2.rectangle(image_blues, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    print(rec_blue[0])\n",
    "    print(rec_blue.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5955cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_blue = image_blues\n",
    "rostro_blue = aux_blue[y:y+h,x:x+w]\n",
    "rostro_blue = cv2.resize(rostro_blue,(150,150),interpolation=cv2.INTER_CUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b55482",
   "metadata": {},
   "outputs": [],
   "source": [
    "rostro_blue.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd7a38b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(rostro_blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bfddf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(image_blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0155a4a5",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a77d53",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8315eda5",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2802b89",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a9365",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737f402e",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa68e81",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc34d3",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441be51e",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918a16c",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3959efe6",
   "metadata": {},
   "source": [
    "### FUNCTION TWO-B ¿?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cd997b",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424cb530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(image_greens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e441e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAY = IMAGE_GREENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(image_greens))\n",
    "print('len(image_greens): ',len(image_greens))\n",
    "print('image_greens.shape: ',image_greens.shape)\n",
    "print('len(image_greens)[0]: ',len(image_greens[0]))\n",
    "print('image_greens[0]: ',image_greens[0])\n",
    "print('image_greens[0][0]: ',image_greens[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99063ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_green = face_cascade.detectMultiScale(\n",
    "    image_greens,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=5,\n",
    "    minSize=(30, 30),\n",
    "    #flags = cv2.CASCADE_SCALE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852b1e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(faces_green))\n",
    "print('len(faces_green): ',len(faces_green))\n",
    "print('faces_green.shape: ',faces_green.shape)\n",
    "print('len(faces_green)[0]: ',len(faces_green[0]))\n",
    "print('faces_green[0]: ',faces_green[0])\n",
    "print('faces_green: ',faces_green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5213a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x, y, w, h) in faces_green:\n",
    "    rec_green = cv2.rectangle(image_greens, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    print(rec_green)\n",
    "    print(rec_green.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa1684",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_green = image_greens\n",
    "rostro_green = aux_green[y:y+h,x:x+w]\n",
    "rostro_green = cv2.resize(rostro_green,(150,150),interpolation=cv2.INTER_CUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c9b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rostro_green.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb6508",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rostro_green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be4f02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(image_greens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63304a4c",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cd3d66",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554f9f40",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fabd508",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f93ef1",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec05f22",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeb67cf",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f47f2d",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ba1dd",
   "metadata": {},
   "source": [
    "### EXTRA STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a623afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pth = \"../imgs/\"\n",
    "file = \"im14\"\n",
    "ext = \".jpg\"\n",
    "ab_pth = pth+file+ext\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a363c7",
   "metadata": {},
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a8dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openImageAndDetectFaces1(path):\n",
    "    image = face_recognition.load_image_file(path)\n",
    "    face_locations = face_recognition.face_locations(image)\n",
    "    try:\n",
    "        print(\"I found {} face(s) in this photograph.\".format(len(face_locations)))\n",
    "        plt.imshow(image)\n",
    "        for face_location in face_locations:\n",
    "            top, right, bottom, left = face_location\n",
    "            face_image = image[top:bottom, left:right]\n",
    "            face_image = cv2.cvtColor(face_image, cv2.COLOR_RGB2GRAY)\n",
    "            pil_image = Image.fromarray(face_image)\n",
    "            #plt.imshow(pil_image)\n",
    "            fig, axarr = plt.subplots(len(face_locations),1)\n",
    "            plt.subplot(121)\n",
    "            plt.imshow(modify_size(pil_image))\n",
    "        #return modify_size(pil_image)\n",
    "    except ValueError as e:\n",
    "        print(f\"No face found\")\n",
    "        \n",
    "        \n",
    "def modify_size(img):\n",
    "    size=(48,48)\n",
    "    convert_from = img.resize(size)\n",
    "    face=np.asarray(convert_from)/255\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad234fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "face = openImageAndDetectFaces1(pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea2c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = \"../imgs/\"\n",
    "file = \"im_10\"\n",
    "ext = \".jpg\"\n",
    "ab_pth = pth+file+ext\n",
    "\n",
    "def openImageAndDetectFaces2(ab_pth):\n",
    "    \n",
    "    image = face_recognition.load_image_file(ab_pth)\n",
    "    face_locations = face_recognition.face_locations(image)\n",
    "    count = 0\n",
    "    try:\n",
    "        print(\"I found {} face(s) in this photograph.\".format(len(face_locations)))\n",
    "        #plt.subplot(121)\n",
    "        plt.imshow(image)\n",
    "        for feis in face_locations:\n",
    "            count+=1\n",
    "            top, right, bottom, left = feis\n",
    "            face_image = image[top:bottom, left:right]\n",
    "            face_image = cv2.cvtColor(face_image, cv2.COLOR_RGB2GRAY)\n",
    "            pil_image = Image.fromarray(face_image)\n",
    "            fig, axarr = plt.subplots(len(face_locations),1)\n",
    "            #plt.subplot(121)\n",
    "            #plt.imshow(image)\n",
    "            plt.subplot(121)\n",
    "            plt.imshow(new_size(pil_image))\n",
    "            #image_name = os.path.join(subfolder, '{:05d}.jpg'.format(i))\n",
    "            #im.save(image_name)\n",
    "            #print(pil_image)\n",
    "            face_in_pic_pth = pth+file+f\"_face{count}\"+ext\n",
    "            cv2.imwrite(face_in_pic_pth, face_image)\n",
    "            print(face_in_pic_pth)\n",
    "            #return new_size(pil_image)\n",
    "    except ValueError as e:\n",
    "        print(f\"No face found\")\n",
    "        return pil_image\n",
    "     \n",
    "        \n",
    "def new_size(img):\n",
    "    size=(48,48)\n",
    "    convert_from = img.resize(size)\n",
    "    face=np.asarray(convert_from)/255\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2869c80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "face = openImageAndDetectFaces2(\"../imgs/im_10.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d5e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT DONE\n",
    "def openImageAndDetectFaces_HAAS(paaath):\n",
    "    for file in sorted(paaath.iterdir()):\n",
    "        if file.suffix == '.jpg':\n",
    "            count = 0\n",
    "            \n",
    "            image = face_recognition.load_image_file(file)\n",
    "            face_locations = face_recognition.face_locations(image)\n",
    "            \n",
    "            \n",
    "            img = cv2.imread(file)\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            auxFrame = img.copy()\n",
    "            \n",
    "            try:\n",
    "                #print(f\"I found {len(face_locations)} face(s) in photograph {file.name}.\")\n",
    "                #plt.subplot(121)\n",
    "                #plt.imshow(image)\n",
    "                \n",
    "                # PRIMER APPROACH\n",
    "                \n",
    "                for feis in face_locations:\n",
    "                    count+=1\n",
    "                    top, right, bottom, left = feis\n",
    "                    face_image = image[top:bottom, left:right]\n",
    "                    face_image = cv2.cvtColor(face_image, cv2.COLOR_RGB2GRAY)\n",
    "                     \n",
    "                    jeanchas_gray = face_image.copy()\n",
    "                    jeanchas_faces = face_cascade.detectMultiScale(gray,1.3,5)\n",
    "                    \n",
    "                    for (x,y,w,h) in jeanchas_faces:\n",
    "                        cv2.rectangle(file, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                        rostro = auxFrame[y:y+h,x:x+w]\n",
    "                        rostro = cv2.resize(rostro,(150,150),interpolation=cv2.INTER_CUBIC)\n",
    "                        file_rostro = file.name.replace(\".\",f\"_rostro{count}.\")\n",
    "                        cv2.imwrite(str(folder_faces/file_rostro), rostro)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    pil_image = Image.fromarray(face_image)\n",
    "                    reshaped_image = modify_size(pil_image)\n",
    "                    #fig, axarr = plt.subplots(len(face_locations),1)\n",
    "                    #plt.subplot(121)\n",
    "                    #plt.imshow(new_size(pil_image))\n",
    "                    f.name.replace(\".\",f\"_face{count}.\")\n",
    "                    file_face = file.name.replace(\".\",f\"_face{count}.\")\n",
    "                    cv2.imwrite(str(folder_faces/file_face), face_image)\n",
    "                    \n",
    "                    file_pil = file.name.replace(\".\",f\"_pil{count}.\")\n",
    "                    file_reshaped = file.name.replace(\".\",f\"_reshaped{count}.\")\n",
    "                    \n",
    "                    \n",
    "                    file_rostro = file.name.replace(\".\",f\"_rostro{count}.\")\n",
    "                    cv2.imwrite(str(folder_faces/file_rostro), rostro)\n",
    "                    #cv2.imwrite(str(folder_faces/file_pil), pil_image)\n",
    "                    #cv2.imwrite(str(folder_faces/file_reshaped), reshaped_image)\n",
    "                    #cv2.imwrite(str(folder_faces/file.name)+f\"_face{count}\", face_image)\n",
    "                \n",
    "                # SEGUNDO APPROACH\n",
    "                \n",
    "                for (x,y,w,h) in file:\n",
    "                    x,y,w,h = rostro2\n",
    "                    cv2.rectangle(file, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                    \n",
    "                    rostro = auxFrame[y:y+h,x:x+w]\n",
    "                    rostro = cv2.resize(rostro,(150,150),interpolation=cv2.INTER_CUBIC)\n",
    "                    file_rostro = file.name.replace(\".\",f\"_rostro{count}.\")\n",
    "                    cv2.imwrite(str(folder_faces/file_rostro), rostro)\n",
    "                    \n",
    "                    #return new_size(pil_image)\n",
    "            except ValueError as e:\n",
    "                print(f\"No face found\")\n",
    "                \n",
    "    return reshaped_image\n",
    "\n",
    "        \n",
    "def modify_size(img):\n",
    "    size=(48,48)\n",
    "    convert_from = img.resize(size)\n",
    "    face=np.asarray(convert_from)/255\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba1d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "openImageAndDetectFaces_HAAS(folder_imgs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iron",
   "language": "python",
   "name": "iron"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
