{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39bccf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "#os.sys.path\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imgplt\n",
    "import code1_data as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3affd3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1_path = Path.home()/'Iron'/'inp1'\n",
    "\n",
    "train_path = inp1_path/'train'\n",
    "\n",
    "testvalidation_path = inp1_path/'test_validation'\n",
    "\n",
    "fer_csv = inp1_path/'Fer.csv'\n",
    "\n",
    "train_csv = train_path/'Training_Data.csv'\n",
    "\n",
    "test_csv = testvalidation_path/'Testing_Data.csv'\n",
    "\n",
    "validation_csv = testvalidation_path/'Validation_Data.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7140daaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fer = pd.read_csv(fer_csv,encoding = \"ISO-8859-1\")\n",
    "df_train = pd.read_csv(train_csv,encoding = \"ISO-8859-1\")\n",
    "df_test = pd.read_csv(test_csv,encoding = \"ISO-8859-1\")\n",
    "df_val = pd.read_csv(validation_csv,encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06f6b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ('angry','disgust','fear','happy','sad','surprise','neutral')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640a8476",
   "metadata": {},
   "source": [
    "## fer2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb8623d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_fer.copy( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1ee7305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35887, 3) 107661 Index(['emotion', 'pixels', 'Usage'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.shape, df.size, df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5959cb39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_size=(48,48)\n",
    "pixels = df['pixels'].tolist() # Converting the relevant column element into a list for each row\n",
    "width, height = 48, 48\n",
    "faces = []\n",
    "\n",
    "for pixel_sequence in pixels:\n",
    "    face = [int(pixel) for pixel in pixel_sequence.split(' ')] # Splitting the string by space character as a list\n",
    "    face = np.asarray(face).reshape(width, height) #converting the list to numpy array in size of 48*48\n",
    "    face = cv2.resize(face.astype('uint8'),image_size) #resize the image to have 48 cols (width) and 48 rows (height)\n",
    "    faces.append(face.astype('float32')) #makes the list of each images of 48*48 and their pixels in numpyarray form\n",
    "\n",
    "faces = np.asarray(faces) #converting the list into numpy array\n",
    "faces = np.expand_dims(faces, -1) #Expand the shape of an array -1=last dimension => means color space\n",
    "emotions = pd.get_dummies(df['emotion']).to_numpy() #doing the one hot encoding type on emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a31de132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 48, 1)\n",
      "(35887, 48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "print(faces[0].shape)\n",
    "print(faces.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37346739",
   "metadata": {},
   "source": [
    "#### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0f62cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize (from 0 to 1)\n",
    "x = faces.astype('float32')\n",
    "x = x / 255.0\n",
    "\n",
    "# scale pixel value (from -1 to 1)\n",
    "x = x - 0.5\n",
    "x = x * 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7ba3b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e7745fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "q,t = emotions.shape\n",
    "q = len(x)\n",
    "q_train = int((0.8) * q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1efbad",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "644702a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "x_train = x[:q_train]\n",
    "y_train = emotions[:q_train]\n",
    "\n",
    "# Validation data\n",
    "x_val = x[q_train:]\n",
    "y_val = emotions[q_train:]\n",
    "\n",
    "data_train = (train_x, train_y)\n",
    "data_val = (val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cae20d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "train_x.shape (pixels):  (28709, 48, 48, 1)\n",
      "\n",
      "\n",
      "train_y.shape: (labels):  (28709, 7)\n",
      "\n",
      "\n",
      "val_x.shape (pixels):  (7178, 48, 48, 1)\n",
      "\n",
      "\n",
      "val_y.shape: (labels):  (7178, 7)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "print('train_x.shape (pixels): ',train_x.shape)  # ==> 4 dims -  no of images , width , height , color\n",
    "print(\"\\n\")\n",
    "print('train_y.shape: (labels): ',train_y.shape)\n",
    "print(\"\\n\")\n",
    "print('val_x.shape (pixels): ',val_x.shape)\n",
    "print(\"\\n\")\n",
    "print('val_y.shape: (labels): ',val_y.shape)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d5d5e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6adadf",
   "metadata": {},
   "source": [
    "## from 2_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3db94397",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LOW REPO:\n",
    "    #https://pub.towardsai.net/step-by-step-guide-in-creating-your-own-emotion-recognition-system-b8aba98134c8\n",
    "    #https://github.com/Nabeel110/Emotion-Recognition\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# importing tensorflow model libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import model_from_json,load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import *\n",
    "import tensorflow.keras.backend as K\n",
    "import json\n",
    "from keras import backend as K\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f56b98d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned_model\n",
    "def base_model():\n",
    "    model = Sequential()\n",
    "    input_shape = (48,48,1)\n",
    "    #1st convolution layer\n",
    "    model.add(Conv2D(64, (5, 5), input_shape=input_shape,activation='relu', padding='same'))\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #2nd convolution layer\n",
    "    model.add(Conv2D(128, (5, 5),activation='relu',padding='same'))\n",
    "    model.add(Conv2D(128, (5, 5),activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #3rd convolution layer\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same'))\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(7))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    my_optimiser = tf.keras.optimizers.Adam(\n",
    "    learning_rate = 0.001, beta_1=0.9, beta_2=0.999, \n",
    "        epsilon=1e-07, amsgrad=False, name='Adam')\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'],\n",
    "                  optimizer=my_optimiser)\n",
    "    \n",
    "    print(model.summary)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38775125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b0fbf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Model.summary of <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f8c75cb37c0>>\n",
      "Epoch 1/15\n",
      "575/575 [==============================] - 1361s 2s/step - loss: 1.7065 - accuracy: 0.3354 - val_loss: 1.5526 - val_accuracy: 0.4040\n",
      "Epoch 2/15\n",
      "575/575 [==============================] - 1283s 2s/step - loss: 1.3771 - accuracy: 0.4708 - val_loss: 1.2891 - val_accuracy: 0.5029\n",
      "Epoch 3/15\n",
      "575/575 [==============================] - 1252s 2s/step - loss: 1.2316 - accuracy: 0.5315 - val_loss: 1.1695 - val_accuracy: 0.5524\n",
      "Epoch 4/15\n",
      "575/575 [==============================] - 1242s 2s/step - loss: 1.1585 - accuracy: 0.5588 - val_loss: 1.1610 - val_accuracy: 0.5532\n",
      "Epoch 5/15\n",
      "575/575 [==============================] - 1236s 2s/step - loss: 1.1076 - accuracy: 0.5777 - val_loss: 1.1464 - val_accuracy: 0.5584\n",
      "Epoch 6/15\n",
      "575/575 [==============================] - 1227s 2s/step - loss: 1.0635 - accuracy: 0.5981 - val_loss: 1.0601 - val_accuracy: 0.6007\n",
      "Epoch 7/15\n",
      "575/575 [==============================] - 1213s 2s/step - loss: 1.0272 - accuracy: 0.6146 - val_loss: 1.0599 - val_accuracy: 0.6070\n",
      "Epoch 8/15\n",
      "575/575 [==============================] - 1199s 2s/step - loss: 0.9872 - accuracy: 0.6283 - val_loss: 1.0647 - val_accuracy: 0.6010\n",
      "Epoch 9/15\n",
      "575/575 [==============================] - 1198s 2s/step - loss: 0.9505 - accuracy: 0.6419 - val_loss: 1.0221 - val_accuracy: 0.6155\n",
      "Epoch 10/15\n",
      "575/575 [==============================] - 1198s 2s/step - loss: 0.9179 - accuracy: 0.6557 - val_loss: 1.0145 - val_accuracy: 0.6173\n",
      "Epoch 11/15\n",
      "575/575 [==============================] - 1195s 2s/step - loss: 0.8907 - accuracy: 0.6659 - val_loss: 1.0310 - val_accuracy: 0.6245\n",
      "Epoch 12/15\n",
      "575/575 [==============================] - 1197s 2s/step - loss: 0.8544 - accuracy: 0.6799 - val_loss: 0.9977 - val_accuracy: 0.6336\n",
      "Epoch 13/15\n",
      "575/575 [==============================] - 1195s 2s/step - loss: 0.8258 - accuracy: 0.6876 - val_loss: 0.9991 - val_accuracy: 0.6383\n",
      "Epoch 14/15\n",
      "575/575 [==============================] - 1193s 2s/step - loss: 0.7914 - accuracy: 0.7044 - val_loss: 0.9942 - val_accuracy: 0.6428\n",
      "Epoch 15/15\n",
      "575/575 [==============================] - 1193s 2s/step - loss: 0.7720 - accuracy: 0.7116 - val_loss: 0.9965 - val_accuracy: 0.6466\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 48, 48, 64)        1664      \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 48, 48, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 24, 24, 128)       204928    \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 24, 24, 128)       409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 7)                 903       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 2,787,015\n",
      "Trainable params: 2,785,863\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = base_model()\n",
    "\n",
    "model_1.fit(x_train, y_train, \n",
    "            validation_data=(x_val, y_val), \n",
    "            epochs=15,\n",
    "            verbose=1, \n",
    "            batch_size=50)\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f4e5e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16202043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b41ac52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f6f71fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.save(\"../src/model_v1_epoch15k.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2cf7cae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 48, 48, 64)        1664      \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 48, 48, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 24, 24, 128)       204928    \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 24, 24, 128)       409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 7)                 903       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 2,787,015\n",
      "Trainable params: 2,785,863\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "07c98754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 83s 368ms/step - loss: 1.0097 - accuracy: 0.6475\n",
      "Accuracy: 64.75%\n",
      "Test loss: 1.0096663236618042\n",
      "Test accuracy: 0.6475341320037842\n"
     ]
    }
   ],
   "source": [
    "scores = model_1.evaluate(x_val, y_val, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d224b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(\"../src/model_v1_epoch15.hdf5\")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath,\n",
    "    monitor='val_acc',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0e8c59a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = Path.cwd().parent/'src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "45a7ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_1 = src_dir/'model_v1_epoch15_2.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ed12c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a525157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_1 = ModelCheckpoint(\n",
    "    filepath_1,\n",
    "    monitor='val_acc',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2078c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_1 = [checkpoint_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a84379b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_1 = 'model_v1_epoch15_2.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "13b6e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model_json = model_1.to_json()\n",
    "with open(name_1+'.json', \"w\") as json_file:\n",
    "    json.dump(model_json, json_file)\n",
    "\n",
    "model_1.save_weights(name_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c62162df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.load_weights(\"../src/model_v1_epoch15k.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851477cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df46e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34939e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0274cddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9604f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254dc85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8894a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb2dc62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac8a5f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa380b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff01345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb7bff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8036088c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "92eb3df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfilepath='Checkpoint_{epoch:02d}_{val_accuracy:.2f}'\\ncheckpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\\ncallbacks_list = [checkpoint]\\n\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "filepath='Checkpoint_{epoch:02d}_{val_accuracy:.2f}'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "caecf5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmoment = time.localtime()\\nname=\\'Model_{}_{}-{}-{}.h5\\'.format(model_1.history.history[\"val_accuracy\"][-1],moment[2],moment[3],moment[4])\\nmodel_1.save(model_v1_epoch15.hdf5)\\n'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "moment = time.localtime()\n",
    "name='Model_{}_{}-{}-{}.h5'.format(model_1.history.history[\"val_accuracy\"][-1],moment[2],moment[3],moment[4])\n",
    "model_1.save(model_v1_epoch15.hdf5)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "33667862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport json\\n\\nmodel_json = model_1.to_json()\\nwith open(name+\\'.json\\', \"w\") as json_file:\\n    json.dump(model_json, json_file)\\n\\nmodel_1.save_weights(name)\\n'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import json\n",
    "\n",
    "model_json = model_1.to_json()\n",
    "with open(name+'.json', \"w\") as json_file:\n",
    "    json.dump(model_json, json_file)\n",
    "\n",
    "model_1.save_weights(name)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "519813bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# If you want to train the same model or try other models, go for this\\n\\n\\nfilepath_1 = src_dir/\\'model_v1_epoch15.hdf5\\n\\ncheckpoint = keras.callbacks.ModelCheckpoint(filepath_1,\\n                                             monitor=\\'val_acc\\',\\n                                             verbose=1,\\n                                             save_best_only=True,\\n                                             mode=\\'max\\')\\ncallbacks = [checkpoint]\\n# if mode == \"train\":\\nmodel.compile(loss=\\'categorical_crossentropy\\',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=[\\'accuracy\\'])\\nnb_train_samples = 28709\\nnb_validation_samples = 3589\\nepochs = 150\\nmodel_info = model.fit_generator(\\n            train_generator,\\n            steps_per_epoch=nb_train_samples // batch_size,\\n            epochs=epochs,\\n            callbacks = callbacks,\\n            validation_data=validation_generator,\\n            validation_steps=nb_validation_samples // batch_size)\\n\\n# plot_model_history(model_info)\\n# model.save_weights(\\'model.h5\\')\\n\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# If you want to train the same model or try other models, go for this\n",
    "\n",
    "\n",
    "filepath_1 = src_dir/'model_v1_epoch15.hdf5\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath_1,\n",
    "                                             monitor='val_acc',\n",
    "                                             verbose=1,\n",
    "                                             save_best_only=True,\n",
    "                                             mode='max')\n",
    "callbacks = [checkpoint]\n",
    "# if mode == \"train\":\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "nb_train_samples = 28709\n",
    "nb_validation_samples = 3589\n",
    "epochs = 150\n",
    "model_info = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=nb_train_samples // batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks = callbacks,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "# plot_model_history(model_info)\n",
    "# model.save_weights('model.h5')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd46ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143673af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6355e245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89213ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a695714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c15224f2",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca71e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iron",
   "language": "python",
   "name": "iron"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
