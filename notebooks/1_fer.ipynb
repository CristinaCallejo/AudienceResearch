{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eac3411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "#os.sys.path\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imgplt\n",
    "import code1_data as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fe917b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1_path = Path.home()/'Iron'/'inp1'\n",
    "\n",
    "train_path = inp1_path/'train'\n",
    "\n",
    "testvalidation_path = inp1_path/'test_validation'\n",
    "\n",
    "fer_csv = inp1_path/'Fer.csv'\n",
    "\n",
    "train_csv = train_path/'Training_Data.csv'\n",
    "\n",
    "test_csv = testvalidation_path/'Testing_Data.csv'\n",
    "\n",
    "validation_csv = testvalidation_path/'Validation_Data.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95634bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fer = pd.read_csv(fer_csv,encoding = \"ISO-8859-1\")\n",
    "df_train = pd.read_csv(train_csv,encoding = \"ISO-8859-1\")\n",
    "df_test = pd.read_csv(test_csv,encoding = \"ISO-8859-1\")\n",
    "df_val = pd.read_csv(validation_csv,encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3624458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ('angry','disgust','fear','happy','sad','surprise','neutral')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95506fd",
   "metadata": {},
   "source": [
    "## fer2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0b80003",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_fer.copy( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31d8171a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35887, 3) 107661 Index(['emotion', 'pixels', 'Usage'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.shape, df.size, df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3562ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_size=(48,48)\n",
    "pixels = df['pixels'].tolist() # Converting the relevant column element into a list for each row\n",
    "width, height = 48, 48\n",
    "faces = []\n",
    "\n",
    "for pixel_sequence in pixels:\n",
    "    face = [int(pixel) for pixel in pixel_sequence.split(' ')] # Splitting the string by space character as a list\n",
    "    face = np.asarray(face).reshape(width, height) #converting the list to numpy array in size of 48*48\n",
    "    face = cv2.resize(face.astype('uint8'),image_size) #resize the image to have 48 cols (width) and 48 rows (height)\n",
    "    faces.append(face.astype('float32')) #makes the list of each images of 48*48 and their pixels in numpyarray form\n",
    "\n",
    "faces = np.asarray(faces) #converting the list into numpy array\n",
    "faces = np.expand_dims(faces, -1) #Expand the shape of an array -1=last dimension => means color space\n",
    "emotions = pd.get_dummies(df['emotion']).to_numpy() #doing the one hot encoding type on emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c38567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 48, 1)\n",
      "(35887, 48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "print(faces[0].shape)\n",
    "print(faces.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c63a47b",
   "metadata": {},
   "source": [
    "### Standardize or Normalize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98232752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize (from 0 to 1)\n",
    "x = faces.astype('float32')\n",
    "x = x / 255.0\n",
    "\n",
    "# scale pixel value (from -1 to 1)\n",
    "x = x - 0.5\n",
    "x = x * 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "764a8b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1acae07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q,t = emotions.shape\n",
    "q = len(x)\n",
    "q_train = int((0.8) * q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8004295a",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebc10157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "x_train = x[:q_train]\n",
    "y_train = emotions[:q_train]\n",
    "\n",
    "# Validation data\n",
    "x_val = x[q_train:]\n",
    "y_val = emotions[q_train:]\n",
    "\n",
    "data_train = (train_x, train_y)\n",
    "data_val = (val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce245eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "train_x.shape (pixels):  (28709, 48, 48, 1)\n",
      "\n",
      "\n",
      "train_y.shape: (labels):  (28709, 7)\n",
      "\n",
      "\n",
      "val_x.shape (pixels):  (7178, 48, 48, 1)\n",
      "\n",
      "\n",
      "val_y.shape: (labels):  (7178, 7)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "print('train_x.shape (pixels): ',train_x.shape)  # ==> 4 dims -  no of images , width , height , color\n",
    "print(\"\\n\")\n",
    "print('train_y.shape: (labels): ',train_y.shape)\n",
    "print(\"\\n\")\n",
    "print('val_x.shape (pixels): ',val_x.shape)\n",
    "print(\"\\n\")\n",
    "print('val_y.shape: (labels): ',val_y.shape)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0c355ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdaae53",
   "metadata": {},
   "source": [
    "## from 2_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01910677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOW REPO:\n",
    "    #https://pub.towardsai.net/step-by-step-guide-in-creating-your-own-emotion-recognition-system-b8aba98134c8\n",
    "    #https://github.com/Nabeel110/Emotion-Recognition\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# importing tesnsorflow model libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import model_from_json,load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0d35db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import json\n",
    "from keras import backend as K\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "903e026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned_model\n",
    "def base_model():\n",
    "    model = Sequential()\n",
    "    input_shape = (48,48,1)\n",
    "    #1st convolution layer\n",
    "    model.add(Conv2D(64, (5, 5), input_shape=input_shape,activation='relu', padding='same'))\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #2nd convolution layer\n",
    "    model.add(Conv2D(128, (5, 5),activation='relu',padding='same'))\n",
    "    model.add(Conv2D(128, (5, 5),activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #3rd convolution layer\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same'))\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(7))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    my_optimiser = tf.keras.optimizers.Adam(\n",
    "    learning_rate = 0.001, beta_1=0.9, beta_2=0.999, \n",
    "        epsilon=1e-07, amsgrad=False, name='Adam')\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'],\n",
    "                  optimizer=my_optimiser)\n",
    "    \n",
    "    print(model.summary)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced144be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a302d7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Model.summary of <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f8c75cb37c0>>\n",
      "Epoch 1/15\n",
      "575/575 [==============================] - 1361s 2s/step - loss: 1.7065 - accuracy: 0.3354 - val_loss: 1.5526 - val_accuracy: 0.4040\n",
      "Epoch 2/15\n",
      "575/575 [==============================] - 1283s 2s/step - loss: 1.3771 - accuracy: 0.4708 - val_loss: 1.2891 - val_accuracy: 0.5029\n",
      "Epoch 3/15\n",
      "575/575 [==============================] - 1252s 2s/step - loss: 1.2316 - accuracy: 0.5315 - val_loss: 1.1695 - val_accuracy: 0.5524\n",
      "Epoch 4/15\n",
      "575/575 [==============================] - 1242s 2s/step - loss: 1.1585 - accuracy: 0.5588 - val_loss: 1.1610 - val_accuracy: 0.5532\n",
      "Epoch 5/15\n",
      "575/575 [==============================] - 1236s 2s/step - loss: 1.1076 - accuracy: 0.5777 - val_loss: 1.1464 - val_accuracy: 0.5584\n",
      "Epoch 6/15\n",
      "575/575 [==============================] - 1227s 2s/step - loss: 1.0635 - accuracy: 0.5981 - val_loss: 1.0601 - val_accuracy: 0.6007\n",
      "Epoch 7/15\n",
      "575/575 [==============================] - 1213s 2s/step - loss: 1.0272 - accuracy: 0.6146 - val_loss: 1.0599 - val_accuracy: 0.6070\n",
      "Epoch 8/15\n",
      "575/575 [==============================] - 1199s 2s/step - loss: 0.9872 - accuracy: 0.6283 - val_loss: 1.0647 - val_accuracy: 0.6010\n",
      "Epoch 9/15\n",
      "575/575 [==============================] - 1198s 2s/step - loss: 0.9505 - accuracy: 0.6419 - val_loss: 1.0221 - val_accuracy: 0.6155\n",
      "Epoch 10/15\n",
      "575/575 [==============================] - 1198s 2s/step - loss: 0.9179 - accuracy: 0.6557 - val_loss: 1.0145 - val_accuracy: 0.6173\n",
      "Epoch 11/15\n",
      "575/575 [==============================] - 1195s 2s/step - loss: 0.8907 - accuracy: 0.6659 - val_loss: 1.0310 - val_accuracy: 0.6245\n",
      "Epoch 12/15\n",
      "575/575 [==============================] - 1197s 2s/step - loss: 0.8544 - accuracy: 0.6799 - val_loss: 0.9977 - val_accuracy: 0.6336\n",
      "Epoch 13/15\n",
      "575/575 [==============================] - 1195s 2s/step - loss: 0.8258 - accuracy: 0.6876 - val_loss: 0.9991 - val_accuracy: 0.6383\n",
      "Epoch 14/15\n",
      "575/575 [==============================] - 1193s 2s/step - loss: 0.7914 - accuracy: 0.7044 - val_loss: 0.9942 - val_accuracy: 0.6428\n",
      "Epoch 15/15\n",
      "575/575 [==============================] - 1193s 2s/step - loss: 0.7720 - accuracy: 0.7116 - val_loss: 0.9965 - val_accuracy: 0.6466\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 48, 48, 64)        1664      \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 48, 48, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 24, 24, 128)       204928    \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 24, 24, 128)       409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 7)                 903       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 2,787,015\n",
      "Trainable params: 2,785,863\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = base_model()\n",
    "\n",
    "model_1.fit(x_train, y_train, \n",
    "            validation_data=(x_val, y_val), \n",
    "            epochs=15,\n",
    "            verbose=1, \n",
    "            batch_size=50)\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c5a3375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 48, 48, 64)        1664      \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 48, 48, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 24, 24, 128)       204928    \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 24, 24, 128)       409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 7)                 903       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 2,787,015\n",
      "Trainable params: 2,785,863\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "23152da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath='Checkpoint_{epoch:02d}_{val_accuracy:.2f}'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2b62368a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 75s 331ms/step - loss: 0.9965 - accuracy: 0.6466\n",
      "Accuracy: 64.66%\n",
      "Test loss: 0.9964521527290344\n",
      "Test accuracy: 0.646558940410614\n"
     ]
    }
   ],
   "source": [
    "scores = model_1.evaluate(x_val, y_val, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b98da2ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-9dcdd31be90f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocaltime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "time.localtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "815c2b3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'History'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-0940f8be4a95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\" metrics collected by history object \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'History'"
     ]
    }
   ],
   "source": [
    "filepath = os.path.join(\"./emotion_detector_models/model_v1_{epoch15}.hdf5\")\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
    "                                             monitor='val_acc',\n",
    "                                             verbose=1,\n",
    "                                             save_best_only=True,\n",
    "                                             mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ed1328e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Checkpoint_{epoch:02d}_{val_accuracy:.2f}'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e11b34b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = Path.cwd().parent/'src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "74bf5a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_1 = src_dir/'model_v1_epoch15.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b614a27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/cris/Iron/ReadingyourAudience/src/model_v1_epoch15.hdf5')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d14af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_1 = ModelCheckpoint(\n",
    "    filepath_1,\n",
    "    monitor='val_acc',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "275a5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_1 = [checkpoint_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2cbfc89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# If you want to train the same model or try other models, go for this\\n\\n\\nfilepath_1 = src_dir/\\'model_v1_epoch15.hdf5\\n\\ncheckpoint = keras.callbacks.ModelCheckpoint(filepath_1,\\n                                             monitor=\\'val_acc\\',\\n                                             verbose=1,\\n                                             save_best_only=True,\\n                                             mode=\\'max\\')\\ncallbacks = [checkpoint]\\n# if mode == \"train\":\\nmodel.compile(loss=\\'categorical_crossentropy\\',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=[\\'accuracy\\'])\\nnb_train_samples = 28709\\nnb_validation_samples = 3589\\nepochs = 150\\nmodel_info = model.fit_generator(\\n            train_generator,\\n            steps_per_epoch=nb_train_samples // batch_size,\\n            epochs=epochs,\\n            callbacks = callbacks,\\n            validation_data=validation_generator,\\n            validation_steps=nb_validation_samples // batch_size)\\n\\n# plot_model_history(model_info)\\n# model.save_weights(\\'model.h5\\')\\n\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# If you want to train the same model or try other models, go for this\n",
    "\n",
    "\n",
    "filepath_1 = src_dir/'model_v1_epoch15.hdf5\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath_1,\n",
    "                                             monitor='val_acc',\n",
    "                                             verbose=1,\n",
    "                                             save_best_only=True,\n",
    "                                             mode='max')\n",
    "callbacks = [checkpoint]\n",
    "# if mode == \"train\":\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "nb_train_samples = 28709\n",
    "nb_validation_samples = 3589\n",
    "epochs = 150\n",
    "model_info = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=nb_train_samples // batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks = callbacks,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "# plot_model_history(model_info)\n",
    "# model.save_weights('model.h5')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f1314b",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e965d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iron",
   "language": "python",
   "name": "iron"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
