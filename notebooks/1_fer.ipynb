{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86018bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "#os.sys.path\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imgplt\n",
    "import code1_data as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44733299",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1_path = Path.home()/'Iron'/'inp1'\n",
    "\n",
    "train_path = inp1_path/'train'\n",
    "\n",
    "testvalidation_path = inp1_path/'test_validation'\n",
    "\n",
    "fer_csv = inp1_path/'Fer.csv'\n",
    "\n",
    "train_csv = train_path/'Training_Data.csv'\n",
    "\n",
    "test_csv = testvalidation_path/'Testing_Data.csv'\n",
    "\n",
    "validation_csv = testvalidation_path/'Validation_Data.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cafbf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fer = pd.read_csv(fer_csv,encoding = \"ISO-8859-1\")\n",
    "df_train = pd.read_csv(train_csv,encoding = \"ISO-8859-1\")\n",
    "df_test = pd.read_csv(test_csv,encoding = \"ISO-8859-1\")\n",
    "df_val = pd.read_csv(validation_csv,encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcf488ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ('angry','disgust','fear','happy','sad','surprise','neutral')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86118341",
   "metadata": {},
   "source": [
    "## fer2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bab50159",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_fer.copy( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e19c3b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35887, 3) 107661 Index(['emotion', 'pixels', 'Usage'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.shape, df.size, df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "092cbc8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_size=(48,48)\n",
    "pixels = df['pixels'].tolist() # Converting the relevant column element into a list for each row\n",
    "width, height = 48, 48\n",
    "faces = []\n",
    "\n",
    "for pixel_sequence in pixels:\n",
    "    face = [int(pixel) for pixel in pixel_sequence.split(' ')] # Splitting the string by space character as a list\n",
    "    face = np.asarray(face).reshape(width, height) #converting the list to numpy array in size of 48*48\n",
    "    face = cv2.resize(face.astype('uint8'),image_size) #resize the image to have 48 cols (width) and 48 rows (height)\n",
    "    faces.append(face.astype('float32')) #makes the list of each images of 48*48 and their pixels in numpyarray form\n",
    "\n",
    "faces = np.asarray(faces) #converting the list into numpy array\n",
    "faces = np.expand_dims(faces, -1) #Expand the shape of an array -1=last dimension => means color space\n",
    "emotions = pd.get_dummies(df['emotion']).to_numpy() #doing the one hot encoding type on emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b128415f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 48, 1)\n",
      "(35887, 48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "print(faces[0].shape)\n",
    "print(faces.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda5e17d",
   "metadata": {},
   "source": [
    "### Standardize or Normalize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1ba142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize (from 0 to 1)\n",
    "x = faces.astype('float32')\n",
    "x = x / 255.0\n",
    "\n",
    "# scale pixel value (from -1 to 1)\n",
    "x = x - 0.5\n",
    "x = x * 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9191940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4586cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "q,t = emotions.shape\n",
    "q = len(x)\n",
    "q_train = int((0.8) * q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125ec2b1",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49fa6029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "x_train = x[:q_train]\n",
    "y_train = emotions[:q_train]\n",
    "\n",
    "# Validation data\n",
    "x_val = x[q_train:]\n",
    "y_val = emotions[q_train:]\n",
    "\n",
    "data_train = (train_x, train_y)\n",
    "data_val = (val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89c94f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "train_x.shape (pixels):  (28709, 48, 48, 1)\n",
      "\n",
      "\n",
      "train_y.shape: (labels):  (28709, 7)\n",
      "\n",
      "\n",
      "val_x.shape (pixels):  (7178, 48, 48, 1)\n",
      "\n",
      "\n",
      "val_y.shape: (labels):  (7178, 7)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "print('train_x.shape (pixels): ',train_x.shape)  # ==> 4 dims -  no of images , width , height , color\n",
    "print(\"\\n\")\n",
    "print('train_y.shape: (labels): ',train_y.shape)\n",
    "print(\"\\n\")\n",
    "print('val_x.shape (pixels): ',val_x.shape)\n",
    "print(\"\\n\")\n",
    "print('val_y.shape: (labels): ',val_y.shape)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84b6d731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40227b39",
   "metadata": {},
   "source": [
    "## from 2_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd6c2737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOW REPO:\n",
    "    #https://pub.towardsai.net/step-by-step-guide-in-creating-your-own-emotion-recognition-system-b8aba98134c8\n",
    "    #https://github.com/Nabeel110/Emotion-Recognition\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# importing tesnsorflow model libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import model_from_json,load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae763585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned_model\n",
    "def base_model():\n",
    "    model = Sequential()\n",
    "    input_shape = (48,48,1)\n",
    "    #1st convolution layer\n",
    "    model.add(Conv2D(64, (5, 5), input_shape=input_shape,activation='relu', padding='same'))\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #2nd convolution layer\n",
    "    model.add(Conv2D(128, (5, 5),activation='relu',padding='same'))\n",
    "    model.add(Conv2D(128, (5, 5),activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #3rd convolution layer\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same'))\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(7))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    my_optimiser = tf.keras.optimizers.Adam(\n",
    "    learning_rate = 0.001, beta_1=0.9, beta_2=0.999, \n",
    "        epsilon=1e-07, amsgrad=False, name='Adam')\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'],\n",
    "                  optimizer=my_optimiser)\n",
    "    \n",
    "    print(model.summary)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5614ebe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Model.summary of <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f8c75cb37c0>>\n",
      "Epoch 1/15\n",
      "575/575 [==============================] - 1361s 2s/step - loss: 1.7065 - accuracy: 0.3354 - val_loss: 1.5526 - val_accuracy: 0.4040\n",
      "Epoch 2/15\n",
      "575/575 [==============================] - 1283s 2s/step - loss: 1.3771 - accuracy: 0.4708 - val_loss: 1.2891 - val_accuracy: 0.5029\n",
      "Epoch 3/15\n",
      "575/575 [==============================] - 1252s 2s/step - loss: 1.2316 - accuracy: 0.5315 - val_loss: 1.1695 - val_accuracy: 0.5524\n",
      "Epoch 4/15\n",
      "575/575 [==============================] - 1242s 2s/step - loss: 1.1585 - accuracy: 0.5588 - val_loss: 1.1610 - val_accuracy: 0.5532\n",
      "Epoch 5/15\n",
      "575/575 [==============================] - 1236s 2s/step - loss: 1.1076 - accuracy: 0.5777 - val_loss: 1.1464 - val_accuracy: 0.5584\n",
      "Epoch 6/15\n",
      "575/575 [==============================] - 1227s 2s/step - loss: 1.0635 - accuracy: 0.5981 - val_loss: 1.0601 - val_accuracy: 0.6007\n",
      "Epoch 7/15\n",
      "575/575 [==============================] - 1213s 2s/step - loss: 1.0272 - accuracy: 0.6146 - val_loss: 1.0599 - val_accuracy: 0.6070\n",
      "Epoch 8/15\n",
      "575/575 [==============================] - 1199s 2s/step - loss: 0.9872 - accuracy: 0.6283 - val_loss: 1.0647 - val_accuracy: 0.6010\n",
      "Epoch 9/15\n",
      "575/575 [==============================] - 1198s 2s/step - loss: 0.9505 - accuracy: 0.6419 - val_loss: 1.0221 - val_accuracy: 0.6155\n",
      "Epoch 10/15\n",
      "575/575 [==============================] - 1198s 2s/step - loss: 0.9179 - accuracy: 0.6557 - val_loss: 1.0145 - val_accuracy: 0.6173\n",
      "Epoch 11/15\n",
      "575/575 [==============================] - 1195s 2s/step - loss: 0.8907 - accuracy: 0.6659 - val_loss: 1.0310 - val_accuracy: 0.6245\n",
      "Epoch 12/15\n",
      "575/575 [==============================] - 1197s 2s/step - loss: 0.8544 - accuracy: 0.6799 - val_loss: 0.9977 - val_accuracy: 0.6336\n",
      "Epoch 13/15\n",
      "575/575 [==============================] - 1195s 2s/step - loss: 0.8258 - accuracy: 0.6876 - val_loss: 0.9991 - val_accuracy: 0.6383\n",
      "Epoch 14/15\n",
      "575/575 [==============================] - 1193s 2s/step - loss: 0.7914 - accuracy: 0.7044 - val_loss: 0.9942 - val_accuracy: 0.6428\n",
      "Epoch 15/15\n",
      "575/575 [==============================] - 1193s 2s/step - loss: 0.7720 - accuracy: 0.7116 - val_loss: 0.9965 - val_accuracy: 0.6466\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 48, 48, 64)        1664      \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 48, 48, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 24, 24, 128)       204928    \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 24, 24, 128)       409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 7)                 903       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 2,787,015\n",
      "Trainable params: 2,785,863\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = base_model()\n",
    "\n",
    "model_1.fit(x_train, y_train, \n",
    "            validation_data=(x_val, y_val), \n",
    "            epochs=15,\n",
    "            verbose=1, \n",
    "            batch_size=50)\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2578429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a5ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca6be6a7",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd3fd48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iron",
   "language": "python",
   "name": "iron"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
