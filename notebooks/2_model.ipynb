{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78e94b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#load the libaray to built the model\\nfrom keras.layers import Activation, Convolution2D, Dropout, Conv2D\\nfrom keras.layers import AveragePooling2D, BatchNormalization\\nfrom keras.layers import GlobalAveragePooling2D\\nfrom keras.models import Sequential\\nfrom keras.layers import Flatten\\nfrom keras.models import Model\\nfrom keras.layers import Input\\nfrom keras.layers import MaxPooling2D\\nfrom keras.layers import SeparableConv2D\\nfrom keras import layers\\nfrom keras.regularizers import l2\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HIGH REPO\n",
    "    # https://github.com/PrudhviGNV/FacialEmotionRecognition-usingCNN/blob/master/Facial_Emotion_Recognition_using_CNN.ipynb\n",
    "    # https://medium.com/@prudhvi.gnv/ultimate-guide-for-facial-emotion-recognition-using-a-cnn-f9239fdc63ad\n",
    "\"\"\"\n",
    "#load the libaray to built the model\n",
    "from keras.layers import Activation, Convolution2D, Dropout, Conv2D\n",
    "from keras.layers import AveragePooling2D, BatchNormalization\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import SeparableConv2D\n",
    "from keras import layers\n",
    "from keras.regularizers import l2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e8c8a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOW REPO:\n",
    "    #https://pub.towardsai.net/step-by-step-guide-in-creating-your-own-emotion-recognition-system-b8aba98134c8\n",
    "    #https://github.com/Nabeel110/Emotion-Recognition\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# importing tesnsorflow model libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import model_from_json,load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6cd1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOW REPO:\n",
    "\n",
    "def base_model_0():\n",
    "    #1st convolution layer\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(\n",
    "        filters = 64, \n",
    "        kernel_size = (3, 3),\n",
    "        activation = 'relu', \n",
    "        input_shape = (X_train.shape[1:])\n",
    "    ))\n",
    "    model.add(Conv2D(\n",
    "        filters = 64,\n",
    "        kernel_size = (3, 3), \n",
    "        activation ='relu'))\n",
    "    model.add(MaxPooling2D(\n",
    "        pool_size = (2, 2), \n",
    "        strides = (2, 2)\n",
    "    ))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #2nd convolution layer\n",
    "    model.add(Conv2D(\n",
    "        filters = 64, \n",
    "        kernel_size = (3, 3), \n",
    "        activation='relu')\n",
    "             )\n",
    "    model.add(Conv2D(\n",
    "        filters = 64, \n",
    "        kernel_size = (3, 3), \n",
    "        activation='relu')\n",
    "             )\n",
    "    model.add(MaxPooling2D(\n",
    "        pool_size = (2, 2), \n",
    "        strides = (2, 2)\n",
    "    ))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #3rd convolution layer\n",
    "    model.add(Conv2D(\n",
    "        filters = 128,\n",
    "        kernel_size = (3, 3), \n",
    "        activation ='relu'))\n",
    "    model.add(Conv2D(\n",
    "        filters = 128,\n",
    "        kernel_size = (3, 3), \n",
    "        activation ='relu'))\n",
    "    model.add(MaxPooling2D(\n",
    "        pool_size = (2, 2), \n",
    "        strides = (2, 2)\n",
    "    ))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    #fully connected neural networks\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_labels, activation='softmax'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4549c9",
   "metadata": {},
   "source": [
    "***The output accuracy was as follows after 30 epochs:***\n",
    "\n",
    "Epoch 30/30\n",
    "\n",
    "449/449 [==============================] - 8s 17ms/step \n",
    "\n",
    "- loss: 1.8105 \n",
    "- accuracy: 0.2513 \n",
    "- val_loss: 1.8126 \n",
    "- val_accuracy: 0.2494"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fb35793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned_model\n",
    "def base_model():\n",
    "    model = Sequential()\n",
    "    input_shape = (48,48,1)\n",
    "    #1st convolution layer\n",
    "    model.add(Conv2D(64, (5, 5), input_shape=input_shape,activation='relu', padding='same'))\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #2nd convolution layer\n",
    "    model.add(Conv2D(128, (5, 5),activation='relu',padding='same'))\n",
    "    model.add(Conv2D(128, (5, 5),activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    #3rd convolution layer\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same'))\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(7))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    my_optimiser = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam')\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=my_optimiser)\n",
    "    \n",
    "    print(model.summary)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "445b07a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncomment from yoiutube:\\nThe original paper where the batch normalization technique \\nwas introduced (by Sergey Ioffe, Christian Szegedy) \\nsays that removing dropout speeds up training, \\nwithout increasing overfitting and there also \\nrecommendations not to use drop out together with batch normalization\\nsince it adds noise to stats calculations (mean and variance)\\n...so should we really use DO with BN?\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "comment from yoiutube:\n",
    "The original paper where the batch normalization technique \n",
    "was introduced (by Sergey Ioffe, Christian Szegedy) \n",
    "says that removing dropout speeds up training, \n",
    "without increasing overfitting and there also \n",
    "recommendations not to use drop out together with batch normalization\n",
    "since it adds noise to stats calculations (mean and variance)\n",
    "...so should we really use DO with BN?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb88e5b5",
   "metadata": {},
   "source": [
    "### overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be35a740",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8d7207e808fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model_1.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_1' is not defined"
     ]
    }
   ],
   "source": [
    "history = model_1.fit(\n",
    "    train_X,     \n",
    "    Y_train, \n",
    "    batch_size=64, \n",
    "    epochs=80, \n",
    "    verbose=1, \n",
    "    validation_data=(val_X,Y_val),\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b71c68",
   "metadata": {},
   "source": [
    "At the end of 80 epochs, \n",
    "we can clearly see that our model has overfitted \n",
    "as our training accuracy keeps on increasing \n",
    "while validation accuracy becomes stagnant at around 0.65."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d942822",
   "metadata": {},
   "source": [
    "***The output accuracy was as follows after 80 epochs:***\n",
    "\n",
    "Epoch 80/80\n",
    "\n",
    "449/449 [==============================]  - 33s 73ms/step \n",
    "\n",
    "- loss: 0.2126 \n",
    "- accuracy: 0.9244 \n",
    "- val_loss: 1.6008 \n",
    "- val_accuracy: 0.6503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34c2101f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-ff57fb1b75b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Let's save this model for further testing:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'base_model(fer).h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_1' is not defined"
     ]
    }
   ],
   "source": [
    "#Let's save this model for further testing:\n",
    "\n",
    "model_1.save('base_model(fer).h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be76b244",
   "metadata": {},
   "source": [
    "**Re-running Tuned Model for 12 epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32ce476b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b4751c727ca4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Running model for 12 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m hist_2 = model_2.fit(\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-71ae3e9b0b13>\u001b[0m in \u001b[0;36mbase_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     ))\n\u001b[1;32m     12\u001b[0m     model.add(Conv2D(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Running model for 12 epochs\n",
    "model_2 = base_model()\n",
    "hist_2 = model_2.fit(\n",
    "    train_X, Y_train,\n",
    "    batch_size=64,\n",
    "    epochs=12,\n",
    "    verbose=1,\n",
    "    validation_data=(val_X, Y_val),\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bcd12c",
   "metadata": {},
   "source": [
    "***The output accuracy was as follows after 12 epochs:***\n",
    "\n",
    "Epoch 12/12\n",
    "449/449 [==============================] - 33s 73ms/step \n",
    "- loss: 0.9368 \n",
    "- accuracy: 0.6493 \n",
    "- val_loss: 1.0876 \n",
    "- val_accuracy: 0.5932"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc16b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iron",
   "language": "python",
   "name": "iron"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
